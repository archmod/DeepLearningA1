{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Deep Learning \n",
    "### Joshua Barry, S3718861"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  View1                 View2                 View3  \\\n",
      "0  Image_000001_001.jpg  Image_000001_002.jpg  Image_000001_003.jpg   \n",
      "1  Image_000002_001.jpg  Image_000002_002.jpg  Image_000002_003.jpg   \n",
      "2  Image_000003_001.jpg  Image_000003_002.jpg  Image_000003_003.jpg   \n",
      "3  Image_000004_001.jpg  Image_000004_002.jpg  Image_000004_003.jpg   \n",
      "4  Image_000005_001.jpg  Image_000005_002.jpg  Image_000005_003.jpg   \n",
      "\n",
      "                  View4     label  \n",
      "0  Image_000001_004.jpg  airplane  \n",
      "1  Image_000002_004.jpg  airplane  \n",
      "2  Image_000003_004.jpg  airplane  \n",
      "3  Image_000004_004.jpg  airplane  \n",
      "4  Image_000005_004.jpg  airplane  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./Data/data.csv\")\n",
    "print(df.head())\n",
    "# Todo split the data reasonably "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device:/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding for the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "1        [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "2        [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "3        [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "4        [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "                               ...                        \n",
      "30525    [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "30526    [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "30527    [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "30528    [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "30529    [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "Name: one_hot, Length: 30530, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "df = pd.read_csv(\"./Data/data.csv\")\n",
    "encoder=OneHotEncoder(sparse=False)\n",
    "df_encoded = pd.DataFrame (encoder.fit_transform(df[['label']]))\n",
    "df_encoded.columns = encoder.get_feature_names_out(['label'])\n",
    "df.drop(['label'] ,axis=1, inplace=True)\n",
    "df_encoded = pd.concat([df, df_encoded ], axis=1)\n",
    "#print(df_encoded.head())\n",
    "\n",
    "labels = ['airplane', 'bathtub', 'bench', 'bottle', 'bus', 'cabinet', 'car', 'chair',\n",
    "                  'display', 'faucet', 'guitar', 'jar', 'lamp', 'laptop', 'loudspeaker', 'rifle', \n",
    "                  'sofa', 'table', 'telephone', 'watercraft']\n",
    "\n",
    "def one_hot(row):\n",
    "    list_ = [row['label_airplane'], row['label_bathtub'], row['label_bench'], row['label_bottle'], row['label_bus'], row['label_cabinet'], \\\n",
    "                row['label_car'], row['label_chair'], row['label_display'], row['label_faucet'], row['label_guitar'], row['label_jar'], \\\n",
    "                row['label_lamp'], row['label_laptop'], row['label_loudspeaker'], row['label_rifle'], row['label_sofa'], row['label_table'], \\\n",
    "                row['label_telephone'], row['label_watercraft']]\n",
    "    row['one_hot'] = list_\n",
    "    return row\n",
    "\n",
    "\n",
    "df_encoded = df_encoded.apply(one_hot, axis=1)\n",
    "print(df_encoded['one_hot'])\n",
    "# df_encoded.drop['']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Test Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24424 6106\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    train_data_df, test_data_df = train_test_split(df_encoded, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "print(train_data_df.shape[0], test_data_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Override methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "tf.Tensor(\n",
      "[[[[255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   ...\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]]\n",
      "\n",
      "  [[255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   ...\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]]\n",
      "\n",
      "  [[255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   ...\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   ...\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]]\n",
      "\n",
      "  [[255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   ...\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]]\n",
      "\n",
      "  [[255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   ...\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]]]\n",
      "\n",
      "\n",
      " [[[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   ...\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   ...\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   ...\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   ...\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   ...\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   ...\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]]\n",
      "\n",
      "\n",
      " [[[255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   ...\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]]\n",
      "\n",
      "  [[255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   ...\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]]\n",
      "\n",
      "  [[255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   ...\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   ...\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]]\n",
      "\n",
      "  [[255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   ...\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]]\n",
      "\n",
      "  [[255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   ...\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   ...\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   ...\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   ...\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   ...\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   ...\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   ...\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]]\n",
      "\n",
      "\n",
      " [[[255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   ...\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]]\n",
      "\n",
      "  [[255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   ...\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]]\n",
      "\n",
      "  [[255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   ...\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   ...\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]]\n",
      "\n",
      "  [[255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   ...\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]]\n",
      "\n",
      "  [[255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   ...\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]]]\n",
      "\n",
      "\n",
      " [[[255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   ...\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]]\n",
      "\n",
      "  [[255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   ...\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]]\n",
      "\n",
      "  [[255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   ...\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   ...\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]]\n",
      "\n",
      "  [[255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   ...\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]]\n",
      "\n",
      "  [[255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   ...\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]]]], shape=(8, 64, 64, 3), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "from scipy.interpolate import interp1d\n",
    "import numpy as np\n",
    "from scipy.ndimage.interpolation import rotate, shift\n",
    "from PIL import Image\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, data_frame, batch_size=8, dim=(32, 32, 3), n_classes=20, data_mean=0, data_std=1,  data_prefix='./Data/Train/', shuffle=True, Augment=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim  # Dimentions of the input\n",
    "        self.batch_size = batch_size\n",
    "        self.n_classes = n_classes  \n",
    "        self.shuffle = shuffle  \n",
    "        self.Augment = Augment  \n",
    "        self.data_frame = data_frame\n",
    "        self.indexes = range(len(df_encoded))\n",
    "        self.image_label = data_frame['one_hot'].values.tolist()\n",
    "        self.image_ids = data_frame[['View1', 'View2', 'View3', 'View4']].values.tolist()\n",
    "        # needs to become a list of list, each list has 4 views for each row\n",
    "        self.data_prefix = data_prefix\n",
    "        \n",
    "        # Data normalization parameters\n",
    "        self.data_mean = data_mean\n",
    "        self.data_std = data_std\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.image_ids) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data for the given index'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        # Find list of IDs\n",
    "        data_ids_temp = [self.image_ids[k] for k in indexes]\n",
    "        image_label_temp = [self.image_label[k] for k in indexes]\n",
    "        \n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(data_ids_temp, image_label_temp)\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.image_ids))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    # Support function\n",
    "    def __data_generation(self, data_ids_temp, image_label_temp):\n",
    "        'Generates data containing batch_size samples' \n",
    "        labels = []\n",
    "        view1 = []\n",
    "        view2 = []\n",
    "        view3 = []\n",
    "        view4 = []    \n",
    "\n",
    "        # Generate data\n",
    "        for i, ids in enumerate(data_ids_temp):\n",
    "            view1.append(self.__read_data_instance(data_ids_temp[i][0]))\n",
    "            view2.append(self.__read_data_instance(data_ids_temp[i][1]))\n",
    "            view3.append(self.__read_data_instance(data_ids_temp[i][2]))\n",
    "            view4.append(self.__read_data_instance(data_ids_temp[i][3]))\n",
    "            labels.append(image_label_temp[i])\n",
    "        views = [view1, view2, view3, view4]\n",
    "        output = [tf.convert_to_tensor(i) for i in views]\n",
    "        print(labels)\n",
    "        return output, np.asarray(labels)\n",
    "\n",
    "\n",
    "    def __read_data_instance(self, label):\n",
    "      # Read an image\n",
    "      filepath = self.data_prefix + label\n",
    "      \n",
    "      data = Image.open(filepath)\n",
    "      #data = data.resize((32,32))\n",
    "      data = np.asarray(data)\n",
    "      #data = np.expand_dims(data, axis=-1) \n",
    "\n",
    "      if self.Augment:\n",
    "          rot = np.random.rand(1) < 0.5\n",
    "          if rot:\n",
    "              rot = np.random.randint(-10,10, size=1)\n",
    "              data = rotate(data, angle=rot[0], reshape=False)\n",
    "          \n",
    "          #shift_val = np.random.randint(-5, high=5, size=2, dtype=int).tolist() + [0,]\n",
    "          #data = shift(data, shift_val, order=0, mode='constant', cval=0.0, prefilter=False)\n",
    "\n",
    "      X = data\n",
    "\n",
    "      # Input normalization\n",
    "      X = (X - self.data_mean)/self.data_std\n",
    "      return X\n",
    "\n",
    "datagen = DataGenerator(df_encoded, batch_size=8, data_mean=0, data_std=1, data_prefix='./Data/Train/', shuffle=True, Augment=True)\n",
    "print(datagen.__getitem__(0)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\DeepLearning\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\rmsprop.py:140: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Input, Concatenate, BatchNormalization, Activation, GlobalAveragePooling2D, Wrapper, Maximum\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "resnet = ResNet50V2(input_shape=(64,64,3), include_top=False, weights='imagenet')\n",
    "\n",
    "for i in resnet.layers:\n",
    "    i.trainable = False\n",
    "\n",
    "class Wrapper(Model):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super(Wrapper, self).__init__()\n",
    "        self.image1 = Input(shape=(64,64,3), batch_size=8)\n",
    "        self.image2 = Input(shape=(64,64,3), batch_size=8)\n",
    "        self.image3 = Input(shape=(64,64,3), batch_size=8)\n",
    "        self.image4 = Input(shape=(64,64,3), batch_size=8)\n",
    "        # need to to inout now \n",
    "\n",
    "        self.resnet = model\n",
    "        self.max_layer = Maximum()\n",
    "        self.GAB = GlobalAveragePooling2D()\n",
    "        self.fc1 = Dense(720, activation='relu')\n",
    "        self.drop1 = Dropout(0.5)\n",
    "        self.fc2 = Dense(512, activation='relu')\n",
    "        self.drop2 = Dropout(0.5)\n",
    "        self.fc3 = Dense(20, activation='softmax')\n",
    "    \n",
    "    def call(self, input):\n",
    "        in1 = input[0]\n",
    "        in2 = input[1]\n",
    "        in3 = input[2]\n",
    "        in4 = input[3]\n",
    "\n",
    "        x1 = self.resnet(in1)\n",
    "        x2 = self.resnet(in2)\n",
    "        x3 = self.resnet(in3)\n",
    "        x4 = self.resnet(in4)\n",
    "\n",
    "        x1 = self.GAB(x1)\n",
    "        x2 = self.GAB(x2)\n",
    "        x3 = self.GAB(x3)\n",
    "        x4 = self.GAB(x4)\n",
    "\n",
    "        x = self.max_layer([x1, x2, x3, x4])\n",
    "        x = self.fc1(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "\n",
    "model = Wrapper(resnet)\n",
    "\n",
    "model.compile(optimizer=RMSprop(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(datagen, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(8, 16384), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "(8, 16384)\n",
      "KerasTensor(type_spec=TensorSpec(shape=(8, 16384), dtype=tf.float32, name=None), name='tf.math.reduce_max/Max:0', description=\"created by layer 'tf.math.reduce_max'\")\n",
      "(8, 16384)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(8, 64, 64, 3)]     0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(8, 64, 64, 3)]     0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(8, 64, 64, 3)]     0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(8, 64, 64, 3)]     0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (8, 64, 64, 32)      896         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (8, 64, 64, 32)      896         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (8, 64, 64, 32)      896         ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (8, 64, 64, 32)      896         ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (8, 32, 32, 32)      0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (8, 32, 32, 32)     0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (8, 32, 32, 32)     0           ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPooling2D)  (8, 32, 32, 32)     0           ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " activation (Activation)        (8, 32, 32, 32)      0           ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (8, 32, 32, 32)      0           ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (8, 32, 32, 32)      0           ['max_pooling2d_4[0][0]']        \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (8, 32, 32, 32)      0           ['max_pooling2d_6[0][0]']        \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (8, 32, 32, 32)      0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (8, 32, 32, 32)      0           ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (8, 32, 32, 32)      0           ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (8, 32, 32, 32)      0           ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (8, 32, 32, 64)      18496       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (8, 32, 32, 64)      18496       ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (8, 32, 32, 64)      18496       ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (8, 32, 32, 64)      18496       ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (8, 16, 16, 64)     0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (8, 16, 16, 64)     0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPooling2D)  (8, 16, 16, 64)     0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPooling2D)  (8, 16, 16, 64)     0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (8, 16, 16, 64)      0           ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (8, 16, 16, 64)      0           ['max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (8, 16, 16, 64)      0           ['max_pooling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (8, 16, 16, 64)      0           ['max_pooling2d_7[0][0]']        \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (8, 16384)           0           ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (8, 16384)           0           ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (8, 16384)           0           ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (8, 16384)           0           ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " tf.math.reduce_max (TFOpLambda  (8, 16384)          0           ['flatten[0][0]',                \n",
      " )                                                                'flatten_1[0][0]',              \n",
      "                                                                  'flatten_2[0][0]',              \n",
      "                                                                  'flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (8, 512)             8389120     ['tf.math.reduce_max[0][0]']     \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (8, 512)             0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (8, 512)             0           ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (8, 1)               513         ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,467,201\n",
      "Trainable params: 8,467,201\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Batch size of 32 is good\n",
    "from turtle import shape\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "# tf.keras.Input\n",
    "from keras import Input as input\n",
    "\n",
    "def createModel():\n",
    "    # 4 inputs, 4 images\n",
    "    input1 = input(shape=(64, 64, 3), batch_size = 8)\n",
    "    input2 = input(shape=(64, 64, 3), batch_size = 8)\n",
    "    input3 = input(shape=(64, 64, 3), batch_size = 8)\n",
    "    input4 = input(shape=(64, 64, 3), batch_size = 8)\n",
    "\n",
    "    convA = Conv2D(32, (3,3), padding=\"same\")(input1)\n",
    "    convA = MaxPooling2D(pool_size=(2, 2))(convA)\n",
    "    convA = Activation(\"relu\")(convA)\n",
    "    # To prevent overfitting\n",
    "    convA = Dropout(0.25)(convA)\n",
    "\n",
    "    convA = Conv2D(64, (3,3), padding=\"same\")(convA)\n",
    "    convA = MaxPooling2D(pool_size=(2, 2))(convA)\n",
    "    convA = Activation(\"relu\")(convA)\n",
    "\n",
    "    convB = Conv2D(32, (3,3), padding=\"same\")(input2)\n",
    "    convB = MaxPooling2D(pool_size=(2, 2))(convB)\n",
    "    convB = Activation(\"relu\")(convB)\n",
    "    convB = Dropout(0.25)(convB)\n",
    "\n",
    "    convB = Conv2D(64, (3,3), padding=\"same\")(convB)\n",
    "    convB = MaxPooling2D(pool_size=(2, 2))(convB)\n",
    "    convB = Activation(\"relu\")(convB)\n",
    "\n",
    "    convC = Conv2D(32, (3,3), padding=\"same\")(input3)\n",
    "    convC = MaxPooling2D(pool_size=(2, 2))(convC)\n",
    "    convC = Activation(\"relu\")(convC)\n",
    "    convC = Dropout(0.25)(convC)\n",
    "\n",
    "    convC = Conv2D(64, (3,3), padding=\"same\")(convC)\n",
    "    convC = MaxPooling2D(pool_size=(2, 2))(convC)\n",
    "    convC = Activation(\"relu\")(convC)\n",
    "\n",
    "    convD = Conv2D(32, (3,3), padding=\"same\")(input4)\n",
    "    convD = MaxPooling2D(pool_size=(2, 2))(convD)\n",
    "    convD = Activation(\"relu\")(convD)\n",
    "    convD = Dropout(0.25)(convD)\n",
    "\n",
    "    convD = Conv2D(64, (3,3), padding=\"same\")(convD)\n",
    "    convD = MaxPooling2D(pool_size=(2, 2))(convD)\n",
    "    convD = Activation(\"relu\")(convD)\n",
    "\n",
    "    # Flatten the data for the dense layers\n",
    "    flatA = Flatten()(convA)\n",
    "    flatB = Flatten()(convB)\n",
    "    flatC = Flatten()(convC)\n",
    "    flatD = Flatten()(convD)\n",
    "\n",
    "    # merge them together\n",
    "    # 4 1d vects\n",
    "    # 1 1d vector preserving as much info as i can \n",
    "    print(flatA)\n",
    "    print(flatA.shape)\n",
    "\n",
    "    # Concatenate the data\n",
    "    merged = tf.math.reduce_max([flatA, flatB, flatC, flatD], axis=0)\n",
    "    print(merged)\n",
    "    print(merged.shape)\n",
    "\n",
    "    # Dense layers\n",
    "    dense = Dense(512)(merged)\n",
    "    dense = Activation(\"relu\")(dense)\n",
    "    dense = Dropout(0.5)(dense)\n",
    "\n",
    "    # Output layer\n",
    "    output = Dense(1, activation=\"sigmoid\")(dense)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[input1, input2, input3, input4], outputs=output)\n",
    "    return model\n",
    "\n",
    "\n",
    "def summary(model):\n",
    "    model.summary()\n",
    "    for layer in model.layers:\n",
    "        print(layer.output_shape)  \n",
    "\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "model = createModel()\n",
    "model.summary()\n",
    "\n",
    "# print(model.layers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\DeepLearning\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001), metrics=['accuracy'])\n",
    "\n",
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(datagen, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global average pooling \n",
    "# flatten\n",
    "# hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(8, 16384), dtype=tf.float32, name=None), name='flatten_4/Reshape:0', description=\"created by layer 'flatten_4'\")\n",
      "(8, 16384)\n",
      "KerasTensor(type_spec=TensorSpec(shape=(8, 16384), dtype=tf.float32, name=None), name='tf.math.reduce_max_1/Max:0', description=\"created by layer 'tf.math.reduce_max_1'\")\n",
      "(8, 16384)\n"
     ]
    }
   ],
   "source": [
    "training_generator = DataGenerator(df_encoded, batch_size=32, data_mean=0, data_std=1, data_prefix='./Data/Train/', shuffle=True, Augment=True)\n",
    "validation_generator = DataGenerator(df_encoded, batch_size=32, data_mean=0, data_std=1, data_prefix='./Data/Train/', shuffle=True, Augment=True)\n",
    "\n",
    "model = createModel()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Save the model\n",
    "model.save('model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(8, 64, 64, 3)]     0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(8, 64, 64, 3)]     0           []                               \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)           [(8, 64, 64, 3)]     0           []                               \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(8, 64, 64, 3)]     0           []                               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (8, 64, 64, 32)      896         ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (8, 64, 64, 32)      896         ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (8, 64, 64, 32)      896         ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (8, 64, 64, 32)      896         ['input_8[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d_8 (MaxPooling2D)  (8, 32, 32, 32)     0           ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_10 (MaxPooling2D  (8, 32, 32, 32)     0           ['conv2d_10[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_12 (MaxPooling2D  (8, 32, 32, 32)     0           ['conv2d_12[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_14 (MaxPooling2D  (8, 32, 32, 32)     0           ['conv2d_14[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (8, 32, 32, 32)      0           ['max_pooling2d_8[0][0]']        \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (8, 32, 32, 32)      0           ['max_pooling2d_10[0][0]']       \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (8, 32, 32, 32)      0           ['max_pooling2d_12[0][0]']       \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (8, 32, 32, 32)      0           ['max_pooling2d_14[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (8, 32, 32, 32)      0           ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (8, 32, 32, 32)      0           ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (8, 32, 32, 32)      0           ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (8, 32, 32, 32)      0           ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (8, 32, 32, 64)      18496       ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (8, 32, 32, 64)      18496       ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (8, 32, 32, 64)      18496       ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (8, 32, 32, 64)      18496       ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_9 (MaxPooling2D)  (8, 16, 16, 64)     0           ['conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_11 (MaxPooling2D  (8, 16, 16, 64)     0           ['conv2d_11[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_13 (MaxPooling2D  (8, 16, 16, 64)     0           ['conv2d_13[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_15 (MaxPooling2D  (8, 16, 16, 64)     0           ['conv2d_15[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (8, 16, 16, 64)      0           ['max_pooling2d_9[0][0]']        \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (8, 16, 16, 64)      0           ['max_pooling2d_11[0][0]']       \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (8, 16, 16, 64)      0           ['max_pooling2d_13[0][0]']       \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (8, 16, 16, 64)      0           ['max_pooling2d_15[0][0]']       \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)            (8, 16384)           0           ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)            (8, 16384)           0           ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_6 (Flatten)            (8, 16384)           0           ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_7 (Flatten)            (8, 16384)           0           ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.reduce_max_1 (TFOpLamb  (8, 16384)          0           ['flatten_4[0][0]',              \n",
      " da)                                                              'flatten_5[0][0]',              \n",
      "                                                                  'flatten_6[0][0]',              \n",
      "                                                                  'flatten_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (8, 512)             8389120     ['tf.math.reduce_max_1[0][0]']   \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (8, 512)             0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (8, 512)             0           ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (8, 1)               513         ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,467,201\n",
      "Trainable params: 8,467,201\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary() # Print the model summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Custom CNN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "\n",
    "    # Initialize components of the model\n",
    "    def __init__(self, filter_num, stride=1, reg_lambda=0.0):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=filter_num,\n",
    "                                            kernel_size=(3, 3),\n",
    "                                            strides=stride,\n",
    "                                            kernel_initializer=\"he_normal\",\n",
    "                                            kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),\n",
    "                                            padding=\"same\")\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization(momentum=.4)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=filter_num,\n",
    "                                            kernel_size=(3, 3),\n",
    "                                            strides=1,\n",
    "                                            kernel_initializer=\"he_normal\",\n",
    "                                            kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),\n",
    "                                            padding=\"same\")\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization(momentum=.4)\n",
    "        if stride != 1:\n",
    "            self.downsample = tf.keras.Sequential()\n",
    "            self.downsample.add(tf.keras.layers.Conv2D(filters=filter_num,\n",
    "                                                       kernel_size=(1, 1),\n",
    "                                                       kernel_initializer=\"he_normal\",\n",
    "                                                       kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),\n",
    "                                                       strides=stride))\n",
    "            self.downsample.add(tf.keras.layers.BatchNormalization(momentum=.4))\n",
    "        else:\n",
    "            self.downsample = lambda x: x\n",
    "\n",
    "    # Define the forward function\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        residual = self.downsample(inputs)\n",
    "\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "\n",
    "        output = tf.nn.relu(tf.keras.layers.add([residual, x]))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'conv1': self.conv1,\n",
    "            'bn1': self.bn1,\n",
    "            'conv2': self.conv2,\n",
    "            'bn2': self.bn2,\n",
    "            'downsample': self.downsample,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_model(filters, block_size, reg_lambda=0.0, fdropout=False):\n",
    "  model = tf.keras.Sequential()\n",
    "\n",
    "  #initial segment\n",
    "  model.add(tf.keras.layers.Conv2D(filters=64,\n",
    "                                   kernel_size=(3, 3),\n",
    "                                   strides=1,\n",
    "                                   kernel_initializer=\"he_normal\",\n",
    "                                   kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),\n",
    "                                   padding=\"same\", input_shape=(32, 32, 3)))\n",
    "  model.add(tf.keras.layers.BatchNormalization(momentum=.4))\n",
    "\n",
    "  #Stack of residual blocks\n",
    "  for nFilters, nBlocks in zip(filters, block_size):\n",
    "    model.add(ResidualBlock(nFilters, stride=2, reg_lambda=reg_lambda))\n",
    "    \n",
    "    for _ in range(1, nBlocks):\n",
    "      model.add(ResidualBlock(nFilters, stride=1, reg_lambda=reg_lambda))\n",
    "\n",
    "  # Final part\n",
    "  model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "  model.add(tf.keras.layers.Dense(10, \n",
    "                                  activation=tf.nn.softmax, \n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),\n",
    "                                  kernel_initializer=\"he_normal\"))\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the model\n",
    "# from keras.models import load_model\n",
    "# model = load_model('model.h5')\n",
    "\n",
    "# # Evaluate the model\n",
    "# score = model.evaluate(, verbose=0)\n",
    "# print('Test loss:', score[0])\n",
    "# print('Test accuracy:', score[1])\n",
    "\n",
    "# # Predict the model\n",
    "# predictions = model.predict_generator(generator=validation_generator)\n",
    "# print(predictions)\n",
    "\n",
    "# # Save the predictions\n",
    "# np.save('predictions.npy', predictions)\n",
    "\n",
    "# # Load the predictions\n",
    "# predictions = np.load('predictions.npy')\n",
    "\n",
    "# # Plot the predictions\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(predictions)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Create RESNOV Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Data Augmentation "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('DeepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36062c478ba8668154df998709043a294048a01eff82153ce1a7725f71761d6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
