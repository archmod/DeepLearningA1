{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Deep Learning \n",
    "### Joshua Barry, S3718861"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  View1                 View2                 View3  \\\n",
      "0  Image_000001_001.jpg  Image_000001_002.jpg  Image_000001_003.jpg   \n",
      "1  Image_000002_001.jpg  Image_000002_002.jpg  Image_000002_003.jpg   \n",
      "2  Image_000003_001.jpg  Image_000003_002.jpg  Image_000003_003.jpg   \n",
      "3  Image_000004_001.jpg  Image_000004_002.jpg  Image_000004_003.jpg   \n",
      "4  Image_000005_001.jpg  Image_000005_002.jpg  Image_000005_003.jpg   \n",
      "\n",
      "                  View4     label  \n",
      "0  Image_000001_004.jpg  airplane  \n",
      "1  Image_000002_004.jpg  airplane  \n",
      "2  Image_000003_004.jpg  airplane  \n",
      "3  Image_000004_004.jpg  airplane  \n",
      "4  Image_000005_004.jpg  airplane  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data.csv\")\n",
    "print(df.head())\n",
    "# Todo split the data reasonably "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device:/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding for the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "1        [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "2        [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "3        [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "4        [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "                               ...                        \n",
      "30525    [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "30526    [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "30527    [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "30528    [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "30529    [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "Name: one_hot, Length: 30530, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "df = pd.read_csv(\"./data.csv\")\n",
    "encoder=OneHotEncoder(sparse=False)\n",
    "df_encoded = pd.DataFrame (encoder.fit_transform(df[['label']]))\n",
    "df_encoded.columns = encoder.get_feature_names_out(['label'])\n",
    "df.drop(['label'] ,axis=1, inplace=True)\n",
    "df_encoded = pd.concat([df, df_encoded ], axis=1)\n",
    "#print(df_encoded.head())\n",
    "\n",
    "labels = ['airplane', 'bathtub', 'bench', 'bottle', 'bus', 'cabinet', 'car', 'chair',\n",
    "                  'display', 'faucet', 'guitar', 'jar', 'lamp', 'laptop', 'loudspeaker', 'rifle', \n",
    "                  'sofa', 'table', 'telephone', 'watercraft']\n",
    "\n",
    "def one_hot(row):\n",
    "    list_ = [row['label_airplane'], row['label_bathtub'], row['label_bench'], row['label_bottle'], row['label_bus'], row['label_cabinet'], \\\n",
    "                row['label_car'], row['label_chair'], row['label_display'], row['label_faucet'], row['label_guitar'], row['label_jar'], \\\n",
    "                row['label_lamp'], row['label_laptop'], row['label_loudspeaker'], row['label_rifle'], row['label_sofa'], row['label_table'], \\\n",
    "                row['label_telephone'], row['label_watercraft']]\n",
    "    row['one_hot'] = list_\n",
    "    return row\n",
    "\n",
    "\n",
    "df_encoded = df_encoded.apply(one_hot, axis=1)\n",
    "print(df_encoded['one_hot'])\n",
    "# df_encoded.drop['']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Test Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24424 6106\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    train_data_df, test_data_df = train_test_split(df_encoded, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "print(train_data_df.shape[0], test_data_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Override methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "from scipy.interpolate import interp1d\n",
    "import numpy as np\n",
    "from scipy.ndimage.interpolation import rotate, shift\n",
    "from PIL import Image\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, data_frame, batch_size=8, dim=(64, 64, 3), n_classes=20, data_mean=0, data_std=1,  data_prefix='./Data/Train/', shuffle=True, Augment=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim  # Dimentions of the input\n",
    "        self.batch_size = batch_size\n",
    "        self.n_classes = n_classes  \n",
    "        self.shuffle = shuffle  \n",
    "        self.Augment = Augment  \n",
    "        self.data_frame = data_frame\n",
    "        self.indexes = range(len(df_encoded))\n",
    "        self.image_label = data_frame['one_hot'].values.tolist()\n",
    "        self.image_ids = data_frame[['View1', 'View2', 'View3', 'View4']].values.tolist()\n",
    "        # needs to become a list of list, each list has 4 views for each row\n",
    "        self.data_prefix = data_prefix\n",
    "        \n",
    "        # Data normalization parameters\n",
    "        self.data_mean = data_mean\n",
    "        self.data_std = data_std\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.image_ids) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data for the given index'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        # Find list of IDs\n",
    "        data_ids_temp = [self.image_ids[k] for k in indexes]\n",
    "        image_label_temp = [self.image_label[k] for k in indexes]\n",
    "        \n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(data_ids_temp, image_label_temp)\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.image_ids))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    # Support function\n",
    "    def __data_generation(self, data_ids_temp, image_label_temp):\n",
    "        'Generates data containing batch_size samples' \n",
    "        labels = []\n",
    "        view1 = []\n",
    "        view2 = []\n",
    "        view3 = []\n",
    "        view4 = []    \n",
    "\n",
    "        # Generate data\n",
    "        for i, ids in enumerate(data_ids_temp):\n",
    "            view1.append(self.__read_data_instance(data_ids_temp[i][0]))\n",
    "            view2.append(self.__read_data_instance(data_ids_temp[i][1]))\n",
    "            view3.append(self.__read_data_instance(data_ids_temp[i][2]))\n",
    "            view4.append(self.__read_data_instance(data_ids_temp[i][3]))\n",
    "            labels.append(image_label_temp[i])\n",
    "        views = [view1, view2, view3, view4]\n",
    "        output = [tf.convert_to_tensor(i) for i in views]\n",
    "        return output, np.asarray(labels)\n",
    "\n",
    "\n",
    "    def __read_data_instance(self, label):\n",
    "      # Read an image\n",
    "      filepath = self.data_prefix + label\n",
    "      \n",
    "      data = Image.open(filepath)\n",
    "      #data = data.resize((224,32))\n",
    "      data = np.asarray(data)\n",
    "      #data = np.expand_dims(data, axis=0) \n",
    "\n",
    "      if self.Augment:\n",
    "          rot = np.random.rand(1) < 0.5\n",
    "          if rot:\n",
    "              rot = np.random.randint(-10,10, size=1)\n",
    "              data = rotate(data, angle=rot[0], reshape=False)\n",
    "          \n",
    "          #shift_val = np.random.randint(-5, high=5, size=2, dtype=int).tolist() + [0,]\n",
    "          #data = shift(data, shift_val, order=0, mode='constant', cval=0.0, prefilter=False)\n",
    "\n",
    "      X = data\n",
    "\n",
    "      # Input normalization\n",
    "      X = (X - self.data_mean)/self.data_std\n",
    "      return X\n",
    "\n",
    "datagen = DataGenerator(df_encoded, batch_size=8, data_mean=0, data_std=1, data_prefix='./Train/', shuffle=True, Augment=True)\n",
    "#print(datagen.__getitem__(0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2799/3816 [=====================>........] - ETA: 4:53 - loss: 11.3508 - accuracy: 0.1601"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Input, Concatenate, BatchNormalization, Activation, GlobalAveragePooling2D, Wrapper, Maximum\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "resnet = ResNet50V2(input_shape=(64,64,3), include_top=False, weights='imagenet')\n",
    "\n",
    "for i in resnet.layers:\n",
    "    i.trainable = False\n",
    "\n",
    "class Wrapper(Model):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super(Wrapper, self).__init__()\n",
    "        self.image1 = Input(shape=(64,64,3), batch_size=8)\n",
    "        self.image2 = Input(shape=(64,64,3), batch_size=8)\n",
    "        self.image3 = Input(shape=(64,64,3), batch_size=8)\n",
    "        self.image4 = Input(shape=(64,64,3), batch_size=8)\n",
    "        # need to to inout now \n",
    "\n",
    "        self.resnet = model\n",
    "        self.max_layer = Maximum()\n",
    "        self.GAB = GlobalAveragePooling2D()\n",
    "        self.dense1 = Dense(720, activation='relu')\n",
    "        self.drop1 = tf.keras.layers.Dropout(0.2)\n",
    "        self.dense2 = Dense(512, activation='relu')\n",
    "        self.drop2 = tf.keras.layers.Dropout(0.2)\n",
    "        self.dense3 = Dense(20, activation='softmax')\n",
    "    \n",
    "    def call(self, input):\n",
    "        in1 = input[0]\n",
    "        in2 = input[1]\n",
    "        in3 = input[2]\n",
    "        in4 = input[3]\n",
    "        x1 = self.resnet(in1)\n",
    "        x2 = self.resnet(in2)\n",
    "        x3 = self.resnet(in3)\n",
    "        x4 = self.resnet(in4)\n",
    "        x1 = self.GAB(x1)\n",
    "        x2 = self.GAB(x2)\n",
    "        x3 = self.GAB(x3)\n",
    "        x4 = self.GAB(x4)\n",
    "        x = self.max_layer([x1, x2, x3, x4])\n",
    "        x = self.dense1(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "\n",
    "model = Wrapper(resnet)\n",
    "model.compile(optimizer=RMSprop(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "tensorboard_callback = TensorBoard(log_dir=\"./tensorboard/experiment2/\")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='./checkpoints/experiment2/',\n",
    "    save_weights_only=True,\n",
    "    monitor='accuracy',\n",
    "    mode='max')\n",
    "early_stop = EarlyStopping(monitor='loss', patience=4, restore_best_weights=True)\n",
    "model.fit(datagen, epochs=10, callbacks=[tensorboard_callback, checkpoint_callback, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size of 32 is good\n",
    "from turtle import shape\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "# tf.keras.Input\n",
    "from keras import Input as input\n",
    "\n",
    "def createModel():\n",
    "    # 4 inputs, 4 images\n",
    "    input1 = input(shape=(64, 64, 3), batch_size = 8)\n",
    "    input2 = input(shape=(64, 64, 3), batch_size = 8)\n",
    "    input3 = input(shape=(64, 64, 3), batch_size = 8)\n",
    "    input4 = input(shape=(64, 64, 3), batch_size = 8)\n",
    "\n",
    "    convA = Conv2D(32, (3,3), padding=\"same\")(input1)\n",
    "    convA = MaxPooling2D(pool_size=(2, 2))(convA)\n",
    "    convA = Activation(\"relu\")(convA)\n",
    "    # To prevent overfitting\n",
    "    convA = Dropout(0.25)(convA)\n",
    "\n",
    "    convA = Conv2D(64, (3,3), padding=\"same\")(convA)\n",
    "    convA = MaxPooling2D(pool_size=(2, 2))(convA)\n",
    "    convA = Activation(\"relu\")(convA)\n",
    "\n",
    "    convB = Conv2D(32, (3,3), padding=\"same\")(input2)\n",
    "    convB = MaxPooling2D(pool_size=(2, 2))(convB)\n",
    "    convB = Activation(\"relu\")(convB)\n",
    "    convB = Dropout(0.25)(convB)\n",
    "\n",
    "    convB = Conv2D(64, (3,3), padding=\"same\")(convB)\n",
    "    convB = MaxPooling2D(pool_size=(2, 2))(convB)\n",
    "    convB = Activation(\"relu\")(convB)\n",
    "\n",
    "    convC = Conv2D(32, (3,3), padding=\"same\")(input3)\n",
    "    convC = MaxPooling2D(pool_size=(2, 2))(convC)\n",
    "    convC = Activation(\"relu\")(convC)\n",
    "    convC = Dropout(0.25)(convC)\n",
    "\n",
    "    convC = Conv2D(64, (3,3), padding=\"same\")(convC)\n",
    "    convC = MaxPooling2D(pool_size=(2, 2))(convC)\n",
    "    convC = Activation(\"relu\")(convC)\n",
    "\n",
    "    convD = Conv2D(32, (3,3), padding=\"same\")(input4)\n",
    "    convD = MaxPooling2D(pool_size=(2, 2))(convD)\n",
    "    convD = Activation(\"relu\")(convD)\n",
    "    convD = Dropout(0.25)(convD)\n",
    "\n",
    "    convD = Conv2D(64, (3,3), padding=\"same\")(convD)\n",
    "    convD = MaxPooling2D(pool_size=(2, 2))(convD)\n",
    "    convD = Activation(\"relu\")(convD)\n",
    "\n",
    "    # Flatten the data for the dense layers\n",
    "    flatA = Flatten()(convA)\n",
    "    flatB = Flatten()(convB)\n",
    "    flatC = Flatten()(convC)\n",
    "    flatD = Flatten()(convD)\n",
    "\n",
    "    # merge them together\n",
    "    # 4 1d vects\n",
    "    # 1 1d vector preserving as much info as i can \n",
    "    print(flatA)\n",
    "    print(flatA.shape)\n",
    "\n",
    "    # Concatenate the data\n",
    "    merged = tf.math.reduce_max([flatA, flatB, flatC, flatD], axis=0)\n",
    "    print(merged)\n",
    "    print(merged.shape)\n",
    "\n",
    "    # Dense layers\n",
    "    dense = Dense(512)(merged)\n",
    "    dense = Activation(\"relu\")(dense)\n",
    "    dense = Dropout(0.5)(dense)\n",
    "\n",
    "    # Output layer\n",
    "    output = Dense(1, activation=\"sigmoid\")(dense)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[input1, input2, input3, input4], outputs=output)\n",
    "    return model\n",
    "\n",
    "\n",
    "def summary(model):\n",
    "    model.summary()\n",
    "    for layer in model.layers:\n",
    "        print(layer.output_shape)  \n",
    "\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "model = createModel()\n",
    "model.summary()\n",
    "\n",
    "# print(model.layers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001), metrics=['accuracy'])\n",
    "\n",
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(datagen, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global average pooling \n",
    "# flatten\n",
    "# hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = DataGenerator(df_encoded, batch_size=32, data_mean=0, data_std=1, data_prefix='./Data/Train/', shuffle=True, Augment=True)\n",
    "validation_generator = DataGenerator(df_encoded, batch_size=32, data_mean=0, data_std=1, data_prefix='./Data/Train/', shuffle=True, Augment=True)\n",
    "\n",
    "model = createModel()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Save the model\n",
    "model.save('model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary() # Print the model summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Custom CNN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "\n",
    "    # Initialize components of the model\n",
    "    def __init__(self, filter_num, stride=1, reg_lambda=0.0):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=filter_num,\n",
    "                                            kernel_size=(3, 3),\n",
    "                                            strides=stride,\n",
    "                                            kernel_initializer=\"he_normal\",\n",
    "                                            kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),\n",
    "                                            padding=\"same\")\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization(momentum=.4)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=filter_num,\n",
    "                                            kernel_size=(3, 3),\n",
    "                                            strides=1,\n",
    "                                            kernel_initializer=\"he_normal\",\n",
    "                                            kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),\n",
    "                                            padding=\"same\")\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization(momentum=.4)\n",
    "        if stride != 1:\n",
    "            self.downsample = tf.keras.Sequential()\n",
    "            self.downsample.add(tf.keras.layers.Conv2D(filters=filter_num,\n",
    "                                                       kernel_size=(1, 1),\n",
    "                                                       kernel_initializer=\"he_normal\",\n",
    "                                                       kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),\n",
    "                                                       strides=stride))\n",
    "            self.downsample.add(tf.keras.layers.BatchNormalization(momentum=.4))\n",
    "        else:\n",
    "            self.downsample = lambda x: x\n",
    "\n",
    "    # Define the forward function\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        residual = self.downsample(inputs)\n",
    "\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "\n",
    "        output = tf.nn.relu(tf.keras.layers.add([residual, x]))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'conv1': self.conv1,\n",
    "            'bn1': self.bn1,\n",
    "            'conv2': self.conv2,\n",
    "            'bn2': self.bn2,\n",
    "            'downsample': self.downsample,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_model(filters, block_size, reg_lambda=0.0, fdropout=False):\n",
    "  model = tf.keras.Sequential()\n",
    "\n",
    "  #initial segment\n",
    "  model.add(tf.keras.layers.Conv2D(filters=64,\n",
    "                                   kernel_size=(3, 3),\n",
    "                                   strides=1,\n",
    "                                   kernel_initializer=\"he_normal\",\n",
    "                                   kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),\n",
    "                                   padding=\"same\", input_shape=(32, 32, 3)))\n",
    "  model.add(tf.keras.layers.BatchNormalization(momentum=.4))\n",
    "\n",
    "  #Stack of residual blocks\n",
    "  for nFilters, nBlocks in zip(filters, block_size):\n",
    "    model.add(ResidualBlock(nFilters, stride=2, reg_lambda=reg_lambda))\n",
    "    \n",
    "    for _ in range(1, nBlocks):\n",
    "      model.add(ResidualBlock(nFilters, stride=1, reg_lambda=reg_lambda))\n",
    "\n",
    "  # Final part\n",
    "  model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "  model.add(tf.keras.layers.Dense(10, \n",
    "                                  activation=tf.nn.softmax, \n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(reg_lambda),\n",
    "                                  kernel_initializer=\"he_normal\"))\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the model\n",
    "# from keras.models import load_model\n",
    "# model = load_model('model.h5')\n",
    "\n",
    "# # Evaluate the model\n",
    "# score = model.evaluate(, verbose=0)\n",
    "# print('Test loss:', score[0])\n",
    "# print('Test accuracy:', score[1])\n",
    "\n",
    "# # Predict the model\n",
    "# predictions = model.predict_generator(generator=validation_generator)\n",
    "# print(predictions)\n",
    "\n",
    "# # Save the predictions\n",
    "# np.save('predictions.npy', predictions)\n",
    "\n",
    "# # Load the predictions\n",
    "# predictions = np.load('predictions.npy')\n",
    "\n",
    "# # Plot the predictions\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(predictions)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Create RESNOV Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Data Augmentation "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93300172903a4a6f189e7c4952e89371dcfbc9e576ebaf7a4f5914444d40577a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
